"""
Agent Fine-tuning and RAG Optimization for LangGraph Portfolio Advisor
Optimizes: workflow execution, RAG retrieval, error handling
"""

import time
import json
import pandas as pd
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass
import logging
from pathlib import Path
import numpy as np
from collections import defaultdict

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class RAGMetrics:
    """Metrics for RAG system evaluation"""
    precision_at_k: float
    recall_at_k: float
    ndcg: float
    mean_reciprocal_rank: float
    avg_retrieval_time: float


class RAGOptimizer:
    """Optimize hybrid retrieval (BM25 + Vector)"""
    
    def __init__(self, vectorstore, documents: List[Dict[str, Any]]):
        self.vectorstore = vectorstore
        self.documents = documents
        
    def grid_search_weights(
        self,
        test_queries: List[Dict[str, Any]],
        weight_range: List[float] = [0.3, 0.5, 0.7, 0.9]
    ) -> Dict[str, Any]:
        """Grid search to find optimal BM25/Vector weights"""
        
        logger.info("Starting grid search for optimal retrieval weights...")
        
        best_score = 0
        best_config = None
        results = []
        
        for vector_weight in weight_range:
            bm25_weight = 1.0 - vector_weight
            
            logger.info(f"Testing weights: Vector={vector_weight:.1f}, BM25={bm25_weight:.1f}")
            
            metrics = self._evaluate_retrieval(
                test_queries,
                vector_weight=vector_weight,
                bm25_weight=bm25_weight
            )
            
            # Composite score
            score = (metrics.precision_at_k * 0.4 + 
                    metrics.recall_at_k * 0.3 + 
                    metrics.ndcg * 0.3)
            
            results.append({
                "vector_weight": vector_weight,
                "bm25_weight": bm25_weight,
                "metrics": metrics,
                "composite_score": score
            })
            
            if score > best_score:
                best_score = score
                best_config = (vector_weight, bm25_weight)
        
        logger.info(f"Best configuration: Vector={best_config[0]:.1f}, BM25={best_config[1]:.1f}")
        logger.info(f"Best composite score: {best_score:.4f}")
        
        return {
            "best_weights": {"vector": best_config[0], "bm25": best_config[1]},
            "best_score": best_score,
            "all_results": results
        }
    
    def _evaluate_retrieval(
        self,
        test_queries: List[Dict[str, Any]],
        vector_weight: float,
        bm25_weight: float,
        k: int = 5
    ) -> RAGMetrics:
        """Evaluate retrieval with given weights"""
        
        from langchain_community.retrievers import BM25Retriever
        from langchain.retrievers import EnsembleRetriever
        from langchain.schema import Document
        
        # Setup retrievers
        docs = [Document(page_content=d["text"], metadata=d.get("metadata", {})) 
                for d in self.documents]
        
        bm25_retriever = BM25Retriever.from_documents(docs)
        bm25_retriever.k = k
        
        vector_retriever = self.vectorstore.as_retriever(search_kwargs={"k": k})
        
        ensemble_retriever = EnsembleRetriever(
            retrievers=[vector_retriever, bm25_retriever],
            weights=[vector_weight, bm25_weight]
        )
        
        # Evaluate
        precisions = []
        recalls = []
        mrr_scores = []
        retrieval_times = []
        
        for query_data in test_queries:
            query = query_data["query"]
            relevant_ids = set(query_data["relevant_doc_ids"])
            
            start = time.time()
            retrieved_docs = ensemble_retriever.get_relevant_documents(query)
            retrieval_time = time.time() - start
            retrieval_times.append(retrieval_time)
            
            retrieved_ids = {doc.metadata.get("id") for doc in retrieved_docs[:k]}
            
            # Precision@k
            true_positives = len(retrieved_ids & relevant_ids)
            precision = true_positives / k if k > 0 else 0
            precisions.append(precision)
            
            # Recall@k
            recall = true_positives / len(relevant_ids) if relevant_ids else 0
            recalls.append(recall)
            
            # MRR
            for idx, doc in enumerate(retrieved_docs):
                if doc.metadata.get("id") in relevant_ids:
                    mrr_scores.append(1 / (idx + 1))
                    break
            else:
                mrr_scores.append(0)
        
        # Calculate nDCG (simplified)
        ndcg = np.mean(precisions)  # Simplified for demonstration
        
        return RAGMetrics(
            precision_at_k=np.mean(precisions),
            recall_at_k=np.mean(recalls),
            ndcg=ndcg,
            mean_reciprocal_rank=np.mean(mrr_scores),
            avg_retrieval_time=np.mean(retrieval_times)
        )
    
    def create_test_queries(self, num_queries: int = 50) -> List[Dict[str, Any]]:
        """Generate test queries with relevance labels"""
        
        # Mock test queries (in production, use expert-labeled data)
        test_queries = [
            {
                "query": "crypto_cap",
                "relevant_doc_ids": ["rule_crypto_1", "rule_crypto_2"],
                "description": "Cryptocurrency allocation limits"
            },
            {
                "query": "max_equity_for_low_risk",
                "relevant_doc_ids": ["rule_equity_1", "rule_risk_1"],
                "description": "Equity limits for low-risk clients"
            },
            {
                "query": "gold_corridor",
                "relevant_doc_ids": ["rule_gold_1", "rule_commodities_1"],
                "description": "Gold allocation boundaries"
            },
            {
                "query": "liquidity requirements high",
                "relevant_doc_ids": ["rule_liquidity_1", "rule_liquidity_2"],
                "description": "High liquidity need constraints"
            },
            {
                "query": "short horizon equity reduction",
                "relevant_doc_ids": ["rule_horizon_1", "rule_equity_2"],
                "description": "Equity adjustment for short time horizons"
            }
        ]
        
        # Expand to desired size
        expanded_queries = []
        for i in range(num_queries):
            expanded_queries.append(test_queries[i % len(test_queries)])
        
        return expanded_queries


class WorkflowProfiler:
    """Profile LangGraph execution performance"""
    
    def __init__(self, workflow_graph):
        self.graph = workflow_graph
        self.metrics = defaultdict(list)
    
    def profile_execution(self, test_inputs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Profile workflow execution on test inputs"""
        
        logger.info(f"Profiling workflow on {len(test_inputs)} test cases...")
        
        for idx, test_input in enumerate(test_inputs):
            logger.info(f"Test case {idx + 1}/{len(test_inputs)}")
            
            # Execute with timing
            node_times = {}
            total_start = time.time()
            
            try:
                # Instrument each node
                result = self._execute_with_timing(test_input, node_times)
                total_time = time.time() - total_start
                
                self.metrics["total_time"].append(total_time)
                self.metrics["success"].append(True)
                
                for node, node_time in node_times.items():
                    self.metrics[f"node_{node}"].append(node_time)
            
            except Exception as e:
                logger.error(f"Test case {idx} failed: {e}")
                self.metrics["success"].append(False)
                self.metrics["errors"].append(str(e))
        
        return self._compute_statistics()
    
    def _execute_with_timing(self, input_data: Dict[str, Any], timings: Dict[str, float]):
        """Execute workflow and record node timings"""
        
        # This is a simplified version - in production, instrument actual LangGraph
        nodes = ["load_client_metadata", "select_boundary_rules", 
                "load_general_strategies", "generate_custom_portfolio"]
        
        for node in nodes:
            start = time.time()
            # Simulate node execution
            time.sleep(0.1)  # Replace with actual node call
            timings[node] = time.time() - start
        
        return {"status": "success"}
    
    def _compute_statistics(self) -> Dict[str, Any]:
        """Compute performance statistics"""
        
        success_rate = sum(self.metrics["success"]) / len(self.metrics["success"]) * 100
        
        stats = {
            "success_rate": success_rate,
            "total_tests": len(self.metrics["success"]),
            "avg_execution_time": np.mean(self.metrics["total_time"]),
            "p50_execution_time": np.percentile(self.metrics["total_time"], 50),
            "p95_execution_time": np.percentile(self.metrics["total_time"], 95),
            "p99_execution_time": np.percentile(self.metrics["total_time"], 99),
            "node_performance": {}
        }
        
        # Node-level stats
        for key, values in self.metrics.items():
            if key.startswith("node_"):
                node_name = key.replace("node_", "")
                stats["node_performance"][node_name] = {
                    "avg_time": np.mean(values),
                    "p95_time": np.percentile(values, 95),
                    "percentage_of_total": np.mean(values) / stats["avg_execution_time"] * 100
                }
        
        return stats
    
    def identify_bottlenecks(self, stats: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Identify performance bottlenecks"""
        
        bottlenecks = []
        
        for node, perf in stats["node_performance"].items():
            if perf["percentage_of_total"] > 30:
                bottlenecks.append({
                    "node": node,
                    "avg_time": perf["avg_time"],
                    "percentage": perf["percentage_of_total"],
                    "severity": "HIGH"
                })
            elif perf["percentage_of_total"] > 15:
                bottlenecks.append({
                    "node": node,
                    "avg_time": perf["avg_time"],
                    "percentage": perf["percentage_of_total"],
                    "severity": "MEDIUM"
                })
        
        return sorted(bottlenecks, key=lambda x: x["percentage"], reverse=True)


class ErrorHandlingTester:
    """Test error recovery and robustness"""
    
    def __init__(self, workflow_graph):
        self.graph = workflow_graph
    
    def test_error_scenarios(self) -> Dict[str, Any]:
        """Test various error scenarios"""
        
        logger.info("Testing error handling scenarios...")
        
        scenarios = [
            {
                "name": "Missing Client ID",
                "input": {"client_id": None},
                "expected": "ValueError"
            },
            {
                "name": "Invalid Client ID",
                "input": {"client_id": "NONEXISTENT"},
                "expected": "ValueError"
            },
            {
                "name": "Empty Market Conditions",
                "input": {"client_id": "CL001", "marketCondtions": ""},
                "expected": "Should handle gracefully"
            },
            {
                "name": "Malformed Rules Response",
                "input": {"client_id": "CL001", "force_rule_error": True},
                "expected": "Should retry or fallback"
            },
            {
                "name": "LLM Timeout",
                "input": {"client_id": "CL001", "simulate_timeout": True},
                "expected": "Should retry with exponential backoff"
            }
        ]
        
        results = []
        
        for scenario in scenarios:
            logger.info(f"Testing: {scenario['name']}")
            
            try:
                result = self.graph.invoke(scenario["input"])
                results.append({
                    "scenario": scenario["name"],
                    "status": "PASSED" if result else "FAILED",
                    "error": None
                })
            except Exception as e:
                results.append({
                    "scenario": scenario["name"],
                    "status": "ERROR",
                    "error": str(e),
                    "expected": scenario["expected"]
                })
        
        passed = sum(1 for r in results if r["status"] == "PASSED")
        
        return {
            "total_scenarios": len(scenarios),
            "passed": passed,
            "failed": len(scenarios) - passed,
            "pass_rate": passed / len(scenarios) * 100,
            "results": results
        }
    
    def test_concurrent_load(self, num_clients: int = 50) -> Dict[str, Any]:
        """Test system under concurrent load"""
        
        logger.info(f"Load testing with {num_clients} concurrent requests...")
        
        import concurrent.futures
        
        client_ids = [f"CL{str(i).zfill(3)}" for i in range(1, num_clients + 1)]
        
        results = []
        start_time = time.time()
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            futures = {
                executor.submit(self._execute_single, client_id): client_id 
                for client_id in client_ids
            }
            
            for future in concurrent.futures.as_completed(futures):
                client_id = futures[future]
                try:
                    result = future.result()
                    results.append({"client_id": client_id, "status": "success", "time": result["time"]})
                except Exception as e:
                    results.append({"client_id": client_id, "status": "failed", "error": str(e)})
        
        total_time = time.time() - start_time
        
        successful = [r for r in results if r["status"] == "success"]
        failed = [r for r in results if r["status"] == "failed"]
        
        return {
            "total_requests": num_clients,
            "successful": len(successful),
            "failed": len(failed),
            "success_rate": len(successful) / num_clients * 100,
            "total_time": total_time,
            "requests_per_second": num_clients / total_time,
            "avg_response_time": np.mean([r["time"] for r in successful]) if successful else 0,
            "p95_response_time": np.percentile([r["time"] for r in successful], 95) if successful else 0
        }
    
    def _execute_single(self, client_id: str) -> Dict[str, Any]:
        """Execute single workflow"""
        start = time.time()
        result = self.graph.invoke({"client_id": client_id})
        return {"result": result, "time": time.time() - start}


class AgentOptimizationPipeline:
    """End-to-end agent optimization pipeline"""
    
    def __init__(self, workflow_graph, vectorstore, documents):
        self.graph = workflow_graph
        self.vectorstore = vectorstore
        self.documents = documents
    
    def run_optimization(self) -> Dict[str, Any]:
        """Run complete optimization pipeline"""
        
        logger.info("=" * 60)
        logger.info("AGENT OPTIMIZATION PIPELINE")
        logger.info("=" * 60)
        
        results = {}
        
        # 1. RAG Optimization
        logger.info("\n[STEP 1] Optimizing RAG Retrieval...")
        rag_optimizer = RAGOptimizer(self.vectorstore, self.documents)
        test_queries = rag_optimizer.create_test_queries(50)
        rag_results = rag_optimizer.grid_search_weights(test_queries)
        results["rag_optimization"] = rag_results
        
        # 2. Workflow Profiling
        logger.info("\n[STEP 2] Profiling Workflow Performance...")
        profiler = WorkflowProfiler(self.graph)
        test_inputs = [{"client_id": f"CL{str(i).zfill(3)}"} for i in range(1, 21)]
        profile_stats = profiler.profile_execution(test_inputs)
        bottlenecks = profiler.identify_bottlenecks(profile_stats)
        results["workflow_profiling"] = {
            "stats": profile_stats,
            "bottlenecks": bottlenecks
        }
        
        # 3. Error Handling Tests
        logger.info("\n[STEP 3] Testing Error Handling...")
        error_tester = ErrorHandlingTester(self.graph)
        error_results = error_tester.test_error_scenarios()
        results["error_handling"] = error_results
        
        # 4. Load Testing
        logger.info("\n[STEP 4] Load Testing...")
        load_results = error_tester.test_concurrent_load(num_clients=50)
        results["load_testing"] = load_results
        
        # 5. Generate Recommendations
        logger.info("\n[STEP 5] Generating Optimization Recommendations...")
        recommendations = self._generate_recommendations(results)
        results["recommendations"] = recommendations
        
        # Save results
        output_file = f"agent_optimization_results_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w') as f:
            json.dump(results, f, indent=2)
        
        logger.info(f"\nâœ“ Optimization complete! Results saved to: {output_file}")
        
        return results
    
    def _generate_recommendations(self, results: Dict[str, Any]) -> List[Dict[str, str]]:
        """Generate optimization recommendations"""
        
        recommendations = []
        
        # RAG recommendations
        rag_best = results["rag_optimization"]["best_weights"]
        recommendations.append({
            "category": "RAG",
            "priority": "HIGH",
            "recommendation": f"Update ensemble weights to Vector={rag_best['vector']:.1f}, BM25={rag_best['bm25']:.1f}",
            "expected_impact": "15-25% improvement in retrieval precision"
        })
        
        # Workflow bottlenecks
        bottlenecks = results["workflow_profiling"]["bottlenecks"]
        for bottleneck in bottlenecks[:3]:
            recommendations.append({
                "category": "Performance",
                "priority": bottleneck["severity"],
                "recommendation": f"Optimize '{bottleneck['node']}' node (currently {bottleneck['percentage']:.1f}% of execution time)",
                "expected_impact": "Potential 20-40% reduction in total execution time"
            })
        
        # Error handling
        error_pass_rate = results["error_handling"]["pass_rate"]
        if error_pass_rate < 90:
            recommendations.append({
                "category": "Reliability",
                "priority": "CRITICAL",
                "recommendation": f"Improve error handling (current pass rate: {error_pass_rate:.1f}%)",
                "expected_impact": "Target 99% reliability"
            })
        
        # Load testing
        load_success = results["load_testing"]["success_rate"]
        if load_success < 95:
            recommendations.append({
                "category": "Scalability",
                "priority": "HIGH",
                "recommendation": f"Improve concurrent handling (current success: {load_success:.1f}%)",
                "expected_impact": "Support 2x traffic with 99% success rate"
            })
        
        return recommendations


if __name__ == "__main__":
    print("Agent Optimization Pipeline Initialized")
    print("\nTo run optimization:")
    print("  from langgraphapp import gapp")
    print("  from langchain_community.vectorstores import Chroma")
    print("  ")
    print("  optimizer = AgentOptimizationPipeline(gapp, vectorstore, documents)")
    print("  results = optimizer.run_optimization()")
